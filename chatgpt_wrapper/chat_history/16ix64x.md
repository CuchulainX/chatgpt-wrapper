Model: Default (GPT-3.5)

> What has been happening is this Slack channel? joined hetzner-cloud-migration-and-config along with 2 others. Should I see more when I login here? Do I need to be added to any particular projects? image.png  @Ed invited you you think we should have some defualt for this nomad count variable if its not in consul, or should we insist that we have a value set for every nomad job hm. the default is set to one in the nomad file, but of course the base job fails if it does not exist in consul. so yes, we can default to 1 if no value in consul can be found.. cool is there a reason you don’t get it from the consul directly in the nomad file? I’m guessing that this needs to change to something new right: Screenshot 2022-09-14 at 17.22.59.png  Pinned by you Pinning some links: Hetzner Infrastructure as Code: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud Hetzner Cloud Timeline: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3288694785/Hetzner+Cloud+Timeline Service Access: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/365985940/Service+Access Not really sure how database address/access works with the new consul system database credentials are in consul kv, e.g. { "key": "skoobe/credentials/hub-services-database-uri", "value": "mysql://hub-services:uJMps0u2sfYua3@mysql-server.service.consul:3306/services?charset=utf8" }, and database is addressd via consul service discovery dns: mysql-server.service.consul where the service name is given by the database server running. fine for non-prod envs at the moment k not sure I am sure the distinction between credentials and configuration in teh key/value store the credentials section was thought of to be put into the vault at some time.. okay. but we would have, for example, services database host in config and graphql username/password for services database in credentials September 15th, 2022 Having trouble getting automated builds to happen on commits. Do I have to manually add a jenkins webhook or something to bitbucket? jennkins is not reachable from outside world, so a push from bitbucket probably does not work 2 replies Last reply 6 months agoView thread Also, is it possible to have multiple jenkins executors running on the same machine. I feel like a lot of time is spent waiting for nomad, which is not using much resopurces? yup, I can up it .. 12 executors now  sweet doing this nomad file migration really is a pain in the butt… but i think services-py is one of the worse though. Probably.. September 27th, 2022 FYI, with the consul repo (skoobe-consul), all changes to the consul KV store need to be done though the repository, any changes that get made in teh consul UI will get rolled back after a deploy September 28th, 2022 @Arndt Jenssen message from nomad: \* Memory oversubscription is not enabled; Task "cover.cover" memory\_max value will be ignored. Update the Scheduler Configuration to allow oversubscription. How to configure nomad? @Andrew Hayward Some call to the nomad API is needed to enable this. Should be in the nomad docs online.. Does the ElasticMQ sqs replacement need a different library to interface than the one we currently use? Are we going to migrate to this before the migration, or keep using sqs and replace later I sugest to keep the current SQS for now - we already have enough on our plate. And its a API compatible to SQS, see here: https://github.com/softwaremill/elasticmq So its actually trivial to change in the future. GitHub GitHub - softwaremill/elasticmq: In-memory message queue with an Amazon SQS-compatible interface. Runs stand-alone or embedded. In-memory message queue with an Amazon SQS-compatible interface. Runs stand-alone or embedded. - GitHub - softwaremill/elasticmq: In-memory message queue with an Amazon SQS-compatible interface. Ru... (62 kB) https://github.com/softwaremill/elasticmq thats what I thought we just have to be a bit careful to use new queue names in hcloud so that we don’t cause any interference with the aws production no statsd running on hcloud yet? yes, it does.. statsd.service.consul:8225 https://nomad-ui-dev.skoobe.online:4443/ui/jobs/statsd@default September 29th, 2022 So i was reading last night and apparently the managing configuration via different branches for different environments is a bad idea, which i understand now. But there is a better way! which is to environment specific folders (as @Arndt Jenssen suggested initially). So now there is a new structure in the repo. A common directory which has a base configuration, and then branch specific overrides. I think this will actually be easier. I was finding it kind of a pain to get the other system correct without accidentally mixing up different environment configurations when merging and copying etc. also, it looks like we have changed the name of staging environment to stg , instead of staging is thios right @Arndt Jenssen? This could cause problems in a few places potentially. just something to look out for if that is the case. Yup, I went for short names.. But since dev environment names should come exclusively from consul - this should be fine .. sure. its just some services have checkis like if $ENVIRONMENT== "staging": … sort of things that we need to look out for bad practise anyway, should be descriptive like $ENABLE\_X or whatever.. yeah for sure. I just know that there are cases of this, that we might run into. what will the production envirnment be called then? prod ? PRD? September 30th, 2022 @Andrew Hayward wrt Jenkinsfiles - have you been able to operate from a single Jenkinsfile per repo? - are the other Jenkinsfile.\* in services-py obsolete? Or do you think I'd fare better with a Jenkinsfile per 'pipeline' : book\_intake, deployment, etc Im bit using them, but we could keep them to set up special cases Well it won't work for automatic deployment system with multiple files @Andrew Hayward In skoobe-consul, the conf file name with the kv pairs will turn into a path element, e.g. dev/skoobe/configuration/account/global.json will become dev/skoobe/configuration/account/global/{key1|key2} ? yes no it will be skoobe/confiugration/account/global/key the current structure it uses the common folder, and then uses the different branch folders to overwrite any diffss does that makes sense> hm. lets try an example. I want to add this key, adressed via nomad templating to the repo: {{ key "skoobe/configuration/account/file-conf" }} How would you do that? (and its a multiline value as well) (and please forget the dev prefix in my first question) make a file /consul/common/configuration/account.json and then have {"file-conf": "$VALUE"} in the file then if the value is different between environments then add /consul/${BRANCH}/configuration/account.json files also with different values I mean /consul/${ENVIRONMENT}/configuration/account.json not sure how the multi-lineness affects things though. might just have to test that out. I dont’t get the distinction between common and $ENV paths.. you can also create a /consul/common/configuration/account/file-conf.txt file with the value in it, and that will also work 2 replies Last reply 6 months agoView thread the $ENV paths overwrite (or merge rather) with the common paths. Most config is the same for all the branches, but if there is differences between environments, then putting a value in the $ENV path with overwirte/add to those in the common path. eg in common/skoobe/deploy/count.json all the values are 1 , but if you a different value for app\_serviecs on staging, you would add that to /stg/skoobe/deploy/count.json ah, now I get it.. I like also the txt file option, which is probably the way to go for multiline values.. FYI: So far so good with Jenkins Plugin for Jetbrains image.png  fun whats it called? "Jenkins Plugin"? must be one of these: image.png  What's this an indication of? Do I need further configuration for consul or is it an outage? + consul kv get -http-addr=null skoobe/configuration/global/docker\_repo\_host\_port Error querying Consul agent: Get "http://null/v1/kv/skoobe/configuration/global/docker\_repo\_host\_port": dial tcp: lookup null on 127.0.0.53:53: server misbehaving \[Pipeline\] } Consul address in nomad. Ahh, you are not using one of the standard branches? branch naming in repository? or the namespace in consul store? ah - after I change skoobe-consul, a script needs to be run to save new values? yes but also, we get the consul address depending on the branch\_name, and if it’s not hcloud-staging or hcloud-dev then it will not get any value. could fix this. Alright. First up consul faked out due to missing ',' Then I'll create branch hub.hcloud-staging I’’m confused. want to have a chat? And impose your confusion on me? Consul KV issues are fixed. Now, I think a hub branch "hcloud-staging" needs to be created - I don't have the rights You don’t have the right to create a branch in hub repo??? Should have the production origin. 1 reply 6 months agoView thread I mean should branch from production branch .. ah, that’s probably it..
> 
> 1 / 1

It seems like you are discussing various aspects of migrating to Hetzner Cloud and working with Consul for managing configurations and credentials. There is also discussion around Nomad, Jenkins, and ElasticMQ. Some key points include:

1.  Migrating to Hetzner Cloud: You are sharing resources like Hetzner Infrastructure as Code and Hetzner Cloud Timeline.
2.  Consul KV store: You mention that changes to the Consul KV store should be done through the repository. Configuration is managed via different branches for different environments, with environment-specific folders.
3.  Nomad: There's a discussion about enabling memory oversubscription and configuring it in Nomad.
4.  Jenkins: You're discussing the use of Jenkinsfiles and the Jenkins Plugin for JetBrains.
5.  ElasticMQ: You talk about using ElasticMQ as an SQS replacement and deciding to stick with the current SQS for now.
6.  Troubleshooting and addressing issues related to Consul KV, branch naming, and permissions.

Feel free to ask for more specific information or clarification on any of these topics.

1 / 1

> What has been happening is this Slack channel? from October 2022 October 4th, 2022 @Andrew Hayward I installed a TCP load balancer for MUX via terraform on Hetzner cloud. See https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud for details.. neat @Arndt Jenssen If i don’t set a count or constraints in nomad it will just deploy to all clients naturally right? Or do you know if theres another nomad command whiech will enforce this Hm, I think that’s depends on the job type. „system“ does always deploy to all instances, whereas “service” to the ones designated with count. The system job does honor the constraints stanza (I think) October 6th, 2022 @Arndt Jenssen did you ever get mux going on hcloud? I keep getting a ” Could not find the services host. error. which looks like its set properly… (edited)  255 app-services.service.consul 9999 /versionapi/;;?\_method=read Its looking for this service, but can’t see it…? should be going on stg from what I can see what are the credentials for bastion and/or the other nodes? I think Andi he may not be back online till tmr - usually catches up with his son sorry, just got back to my laptop and I’m smashed. will answer tomorrow.. October 7th, 2022 ok, got it. I switched from a static to a variable port but it needs a static one, since path based routing is not defined. so either one needs to define all the paths for traefik in the nomad file or switch back to static port. will try the latter first.. Screenshot 2022-10-07 at 09.52.46.png  I copied your account work and added althe mux conf file to consul KV, if you need to change the mux config 1 reply 6 months agoView thread Also, for Docker to have access to this file it needs to get put into a special place, which you can see in the mux hcloud nomad file The first new production database server is ready. See here for details: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers I’ll give you access rights on separate channels. Regarding the new production database servers, I ordered the second one today and changed my mind regarding configuration a bit. The primary server has now a whopping 320 GB of RAM + 3.84 TB of raid 1 SSD for mysql data + 960 GB raid 1 SSD for binary logs (probably overkill). The replica or secondary has 256 GB of RAM, 3.84 TB raid 1 SSD + 16 TB raid 1 HD for backups. So backups will be done against the replica, which is better anyway. The replica will also serve as a RO instance for reports and stats. Will write this up in https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers October 11th, 2022 Second server is configured. The second one is the primary db server. All done except for the RAID 1 of the secondary SSDs on both servers, this I will do tomorrow. Same credentials as for the first configured server: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers secondary raid on both servers created, this concludes the initial setup. will add a setup description for manual raid configuration to the docs.. October 12th, 2022 I’m in the office trying to push a Docker image for Hub to Hetzner, but I get various errors related to timeouts - I find nothing much on resolving this online. Do I miss something? % docker image tag hub/python3.10-no-corelib:latest 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": unable to connect to 10.0.50.2:5000. Do you need an HTTP proxy? % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": context deadline exceeded (Client.Timeout exceeded while awaiting headers) % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 1 reply 6 months agoView thread Those are internal IP address. Gotta log into bastion and make a tunnel or something. If you're trying to do it from outside. But why not using Jenkins for this I was in VPN after first attempt Does it work like that? I don't know. I tried to build a Hub image within the repo - cancelled after 20mins building - thought it pointed to need to have it pre-built in container repository Well you can put that in the Jenkins pipeline anyway, it should not change between builds if the dicker file doesbt change Dicker\* Docker\* Autocorrector damnit Lil Lol October 13th, 2022 FYI: the docker registry is available via (wireguard) VPN as well: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud -> “Private Docker Registry” (edited)  Could I have a login to access both: Nomad: https://nomad-ui-dev.skoobe.online:4443/ui/jobs Consul: https://consul-ui-dev.skoobe.online:4443/ui/dc1/services they are all the same as staging I think omad nomad:sdhK2S\_A.13Du54 for all of them This too? Dashboard (basic auth protected): https://opensearch-ui-dev.skoobe.online:4443/app/home#/ dont know Great thanks October 21st, 2022 FYI There's a range tree instruction that ingests all keys from subdirs you may find useful {{ range tree "skoobe/configuration/queues/sqs" }} {{ .Key | toUpper }}={{ .Value }} {{ end }} oh neat might need a tweak - it's rendering to local/env with the file as a prefix ENVIRONMENT/ENVIRONMENT=production ENVIRONMENT/SERVER\_ENVIRONMENT=staging ENVIRONMENT/SERVICES\_ENV=STAGING Still cool to nknw October 26th, 2022 @Arndt Jenssen I'm trying to use the nomad template {source = construct in place of  template {date <<-EOF   as many of the hub job will share a similar setup template { # disable\_file\_sandbox = true source = "local/hub-nomad-tpl" # \* Must specify a destination for the template destination = "local/env" env = true } however I'm at pains to get the .tpl file to the ephemeral disk I have a preceding stanza with artifact { source = "https://bb\_user:app\_password@bitbucket.org/skoobe/skoobe-hub/src/hcloud-stg/nomad/hcloud/nomad.tpl" destination = "local/hub-nomad-tpl" } https://bb\_user:app\_password@bitbucket.org/skoobe/skoobe-hub/src/hcloud-stg/nomad/hcloud/nomad.tpl is fine from a browser (already logged in), could a solution be for a skoobe bitbucket app password with read rights on skoobe-hub or something else? Source file in Skoobe / skoobe-hub nomad/hcloud/nomad.tpl Lines Size 2.22 kB 1  data = <<-EOF 2   3                {{ range tree "skoobe/configuration/global" }} 4                {{ .Key | toUpper 5  ... skoobe-hub | Added by Bitbucket Cloud hm, no idea why this is not working. bitbucket app passwords should be fine, altough the jenkins instances are using ssh keys for bitbucket access. I added you to the bitbucket admin group for your further investigation pleasure, but I’d suggest not to spend too much time on it and use the things that are already working. I have hub-ui running, but it's unreachable I figure now I needed a 00-traefik-stg.nomad entry # for HUB UI 4556 "--entryPoints.hub-ui.address=:4556", what's the procedure for doing a rolling deployment of Traefik you shouldn’t need that i think, we don’t declare a service for hub i dont think we just need to expose the port, 4556 in the network stanza right>? or maybe the task stanza network { port "http" { static = 4556 host\_network = "private" } } inside the group staza and then host = "nom.cli-stg-1.skoobe.online"? 2 replies Last reply 5 months agoView thread and this for the service stanza : service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", \] check { type = "tcp" interval = "10s" timeout = "2s" } I think that should workl even better would be to use a real hostname, but this needs to be created in DNS via terraform config. then one could do this in the hub-ui service stanza (and can get rid of the static port) service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", "traefik.http.routers.${NOMAD\_JOB\_NAME}.rule=Host(\`hub-ui-${ENVIRONMENT}.skoobe.online\`)" \] have to change traisn now… Will create that this evening, also for account-ui. Just go with the static port option first .. That's what I did It's not hard to change later created hub-ui-\[env\] and account-ui-\[env\] dns hosts.. 3 replies Last reply 5 months agoView thread October 27th, 2022 anyone had this error in nomad before trying to launch a job: Missing: kv.block(skoobe/credentials/aws-access-key), kv.block(skoobe/credentials/aws-region), kv.block(skoobe/credentials/aws-secret-key), and 1 more (pretty sure these keys are in the consul) nevermind. had - instead of \_ (dumb guy) wrt hub-ui I have issues getting my head round the operation of Traefik and the tag interpolation for it The job is successfully deployed - job shows ip4 to be http://195.201.141.25:8060 .nomad file stanza is: resources { cpu = 100 memory = 1024 network { port "http" { static = 8060 host\_network = "private" } } } service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", "traefik.http.routers.${NOMAD\_JOB\_NAME}.rule=Host(\`hub-ui-${ENVIRONMENT}.skoobe.online\`)" \] Traefik shows what appears to be a valid routing entry (img2) I cannot access Ui either at http://hub-ui-stg.skoobe.online:8060/ or http://195.201.141.25:8060 When hub-ui entry was added to common.tfvars   - has a rollout taken place? subdomain\_prefixes = \["nomad-ui", "consul-ui", "traefik-ui", "sqs-ui", "opensearch-ui", "carbon-ui", "mysql", "app-services", "account-api", "www", "account-ui", "hub-ui"\] 3 files  has the hub-ui nomad config still a static port assigned? ah, yes, can see it above change “static = 8060” to “to = 8060" - then you could access it via http://hub-ui-stg.skoobe.online October 28th, 2022 is the whole hcloud stg environment broken for you guys? I ran the traefik nomad job to add: # 4558: App Services for HUB "--entryPoints.hub-app-services.address=:4558", I confirmed that it was added to the dashboard, so I assumed all was well - then went for lunch. I can connect to the DBs seems to be up again @Arndt Jenssen Did you add some external entrypoint for mux? no, because it is TLS terminating itself. it runs against its own Hetzner load balancer which round-robins to all nomad instances (so it should run as a system type job). its descibed on https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud under “MUX access” so it bypasses Traefik completely.. https://console.hetzner.cloud/projects/1297723/loadbalancers
> 
> 1 / 1

In this Slack channel, there have been various discussions and updates related to the technical infrastructure and projects. Some notable topics include:

1.  Installing a TCP load balancer for MUX via Terraform on Hetzner cloud.
2.  Setting up and configuring new production database servers.
3.  Issues related to pushing Docker images to Hetzner and using Jenkins for builds.
4.  Accessing the private Docker registry via Wireguard VPN.
5.  Using Nomad templates and Bitbucket app passwords.
6.  Deploying and accessing Hub-UI with Traefik and Nomad.
7.  Creating DNS hosts for Hub-UI and Account-UI.
8.  Modifying Traefik entrypoints for Hub and other services.

Throughout the conversation, team members share advice, troubleshooting tips, and code snippets to help each other with the various challenges they encounter.

1 / 1

> Please summarise the these slack channel conversations from November 2022. Here they are: November 2nd, 2022 Morning. I added this test to Jenkinsfile to check for deployment of Hub UI stage("Test Hub UI has deployed") { steps { script { final String url = "http://hub-ui-${ENVIRONMENT}.skoobe.online:4556" final String response = sh(script: "curl -s $url", returnStdout: true).trim() echo "Received response: " + response } } } I'm getting a connection refused error, progress from Friday. If this a host\_network setting error perhaps - currently it's "private" pushed hub-ui port based routing in nomad file to hcloud-stg added terraform config for new services in hetzner load balancer and added ip whitelisting plus proxy protocol config in traefik for hub-ui, acciunt-ui and app-services hub. live on stg already and it works for hub-ui   https://nomad-stg-vpn.skoobe.online:4556/dashboard on dev I need to upgrade the hetzner load balancer to a “better” version, beause the current one does only allow a max of 5 services.. Damn.. I briefly saw the dashboard startup in the logs at - but couldn't figure how it appeared -thought it was the curl test after deployment. I then tried to alter the load balancer service from https to http but it's not showing healthy again, I'm receiving a 503 Service Unavailable error. I seeing a repeating /ping to it that returns 404 Have I broke the LB setup? 3 files  what did change? @Ed why did you redeploy traefik? I made an error with the LB, I thought it needed to execute again to sync did you pull the latest changes I made? Yep hm. I’ll redeploy trafik.. @Ed did you manually changed the port value? it should be 80, once I reverted all is fine again.. (edited)  Screenshot 2022-11-02 at 16.47.05.png  K Tada. I see - there was contention between the main routing and the ping or some such image.png  for your understanding: hetzner load balancer services need just to know if a server is reachable for it to take into its round robin distribution. traefik runs on every instance and has a /ping endpoint - that is what is checked here… Aha. The fog is clearing. The DB schema is out of sync though - it's missing category.child\_safe at least. I guess there's Alembic scripts around - I'll execute them against STG November 3rd, 2022 The first of the Hub workers requiring statsd is complaining It looks as if the variable interpolation in the jobs never occurred - this then leads to the service not being properly 'advertised' to Consul / Traefik - amirite? I would change both the job defs for statsd and carbon if this is the case. Unless I'm on the wrong track - are they working for either of you? 4 files  Why are the variables not getting subsituted? Perhaps when the job is copied and pasted into the the Nomad UI - rather than when there is a Jenkins build   They're running since Jun/Jul checking, but deploying in the nomad ui is the same as deploying it with the nomad binary. The interpolation happens on nomad servers side in both cases. The ENVIRONMENT var remains image.png  I worked out what happened - the appearance of ${VAR\_NAME} in either Nomad UI or in Traefik is a Red Herring - they have been interpolated. 2 replies Last reply 5 months agoView thread The root cause was a straggling reference to statsd.internal.soobe and not statsd.service.consul November 4th, 2022 Gained some momentum this morning. Hub UI: working Book Intake: all working Book Deployment: Long Running Job, Status, Error digest; hub-services-sync\* Spiegel worker looks like we need to be whitelisted INFO: spiegel\_bestseller:106 in fetch\_updates: Downloading bestsellers from https://bestseller.buchreport.de/v1/xml/80/ DEBUG: connectionpool:939 in \_new\_conn: Starting new HTTPS connection (1): bestseller.buchreport.de:443 DEBUG: connectionpool:433 in \_make\_request: https://bestseller.buchreport.de:443 "GET /v1/xml/80/ HTTP/1.1" 200 59 ERROR: spiegel\_bestseller:122 in fetch\_updates: Permission denied for bestseller download: No Data for IP 195.201.141.52 and ID 80 I'll continue with the Kobo pipeline  November 8th, 2022 FYI. I have jobs for ES that can't be changed from Service to Batch, previously the UI had a GC function. Should there be a background Nomad process that should be running to do it? I've changed job names as a workaround. image.png  1 reply 5 months agoView thread id you kill the jiob you can call the nomad api to purge the job (i forgot the exact command) and then it will be gone, and you can then deploy a version with a different type Something stage happening in Consul - many of the jobs are failing receiving a kv.list error (although keys are there) then this is followed by a warning about rate limiting calls image.png  new stage? looks that can be fixed via config: https://discuss.hashicorp.com/t/your-ip-is-issuing-too-many-concurrent-connections-please-rate-limit-your-calls/20711 HashiCorp Discuss Your IP is issuing too many concurrent connections, please rate limit your calls When i am using the REST API this error pops up. Is there any way to fix it? Consul 1.9.0 Reading time 1 mins  Likes 1  Feb 9th, 2021 will up this parameter after the meetings.. @Arndt Jenssen Hi Andi, have you had the chance to do this? - I'm not noticing a difference if you have not yet, its a bit more complicated, but I’ll change it on the fly to see if this solves the problem.. done on stg, please check if that helps. for reference: sudo vi /etc/consul.d/consul.hcl limits { http\_max\_conns\_per\_client = 1000 } sudo service consul restart have to code this up though, since the consul config is currently autogenerated from hashi-up.. could also be necessary on the consul clients side, I just added it to the masters.. November 9th, 2022 @Ed added this to the clients as well.. November 14th, 2022 @Ed Regarding the hub-search-sync-xxx jobs on new infra: Startup fails with: ModuleNotFoundError: No module named 'hub' Traceback (most recent call last): File "./nomad/\_nomad\_daemons/skoobe-hub-es-no-lock-syncd", line 8, in <module> import hub.general\_options Should be easy to fix I guess.. I need a docker image with the corelib exes on Hetzner. I was assuming alpine-python-3.7-root-non-prod contained them, but when I resolved Traefik issues I see/skoobe-epub is not in path Exception: Could not find /skoobe-epub in the path I'm unable to get the Docker image to build - I think it may be down to the M1 chipset  1 reply 3 months agoView thread skoobe-workers are all running, but using hub's ubuntu-python3.8 image for now. I have the feeling it's far more bloated than the alpine one was though (edited)  November 22nd, 2022 Could I have a dbowner account on mysql-stg.skoobe.online db server? I need to setup rtmanager db and grant rights to rtm account etc November 30th, 2022 @Arndt Jenssen is you see this - I would need credentials for Hazelcast on Hetzner to login to Management Centre in order to troubleshoot Open Search from Hub. Tnx
> 
> 2 / 2

The previous model used in this conversation is unavailable. We've switched you to the latest default model

Summary of Slack Channel Conversations from November 2022:

November 2nd:

*   Added a test to Jenkinsfile to check for deployment of Hub UI, getting connection refused error
*   Pushed hub-ui port based routing in nomad file to hcloud-stg
*   Added terraform config for new services in Hetzner load balancer and added IP whitelisting plus proxy protocol config in traefik for hub-ui, account-ui, and app-services hub. Works for hub-ui on stg already
*   Need to upgrade hetzner load balancer to a better version on dev
*   Tried to alter the load balancer service from https to http, but it's not showing healthy again, receiving 503 Service Unavailable error
*   Root cause was a straggling reference to statsd.internal.skoobe and not statsd.service.consul
*   DB schema is out of sync, missing category.child\_safe

November 3rd:

*   Hub worker requiring statsd is complaining, variable interpolation in the jobs never occurred
*   The appearance of ${VAR\_NAME} in either Nomad UI or in Traefik is a Red Herring, they have been interpolated
*   Root cause was a straggling reference to statsd.internal.skoobe and not statsd.service.consul

November 4th:

*   Working on Kobo pipeline, Spiegel worker looks like we need to be whitelisted
*   Jobs for ES can't be changed from Service to Batch, previously the UI had a GC function
*   Something stage happening in Consul, many jobs are failing receiving a kv.list error, followed by a warning about rate limiting calls
*   Added http\_max\_conns\_per\_client parameter to consul config on stg, need to code this up since the consul config is currently autogenerated from hashi-up
*   Added http\_max\_conns\_per\_client parameter to consul config on clients as well

November 9th:

*   Added http\_max\_conns\_per\_client parameter to consul config on clients as well

November 14th:

*   Startup fails with ModuleNotFoundError for hub-search-sync-xxx jobs on new infra
*   Need a Docker image with the corelib exes on Hetzner, unable to get Docker image to build

November 22nd:

*   Requested dbowner account on mysql-stg.skoobe.online db server to setup rtmanager db and grant rights to rtm account

November 30th:

*   Requested credentials for Hazelcast on Hetzner to login to Management Centre in order to troubleshoot Open Search from Hub

1 / 1