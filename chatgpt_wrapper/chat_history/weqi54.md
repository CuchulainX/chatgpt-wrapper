Model: GPT-4

> What has been happening is this Slack channel? from October 2022 October 4th, 2022 @Andrew Hayward I installed a TCP load balancer for MUX via terraform on Hetzner cloud. See https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud for details.. neat @Arndt Jenssen If i don’t set a count or constraints in nomad it will just deploy to all clients naturally right? Or do you know if theres another nomad command whiech will enforce this Hm, I think that’s depends on the job type. „system“ does always deploy to all instances, whereas “service” to the ones designated with count. The system job does honor the constraints stanza (I think) October 6th, 2022 @Arndt Jenssen did you ever get mux going on hcloud? I keep getting a ” Could not find the services host. error. which looks like its set properly… (edited)  255 app-services.service.consul 9999 /versionapi/;;?\_method=read Its looking for this service, but can’t see it…? should be going on stg from what I can see what are the credentials for bastion and/or the other nodes? I think Andi he may not be back online till tmr - usually catches up with his son sorry, just got back to my laptop and I’m smashed. will answer tomorrow.. October 7th, 2022 ok, got it. I switched from a static to a variable port but it needs a static one, since path based routing is not defined. so either one needs to define all the paths for traefik in the nomad file or switch back to static port. will try the latter first.. Screenshot 2022-10-07 at 09.52.46.png  I copied your account work and added althe mux conf file to consul KV, if you need to change the mux config 1 reply 6 months agoView thread Also, for Docker to have access to this file it needs to get put into a special place, which you can see in the mux hcloud nomad file The first new production database server is ready. See here for details: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers I’ll give you access rights on separate channels. Regarding the new production database servers, I ordered the second one today and changed my mind regarding configuration a bit. The primary server has now a whopping 320 GB of RAM + 3.84 TB of raid 1 SSD for mysql data + 960 GB raid 1 SSD for binary logs (probably overkill). The replica or secondary has 256 GB of RAM, 3.84 TB raid 1 SSD + 16 TB raid 1 HD for backups. So backups will be done against the replica, which is better anyway. The replica will also serve as a RO instance for reports and stats. Will write this up in https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers October 11th, 2022 Second server is configured. The second one is the primary db server. All done except for the RAID 1 of the secondary SSDs on both servers, this I will do tomorrow. Same credentials as for the first configured server: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3346726913/Production+database+servers secondary raid on both servers created, this concludes the initial setup. will add a setup description for manual raid configuration to the docs.. October 12th, 2022 I’m in the office trying to push a Docker image for Hub to Hetzner, but I get various errors related to timeouts - I find nothing much on resolving this online. Do I miss something? % docker image tag hub/python3.10-no-corelib:latest 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": unable to connect to 10.0.50.2:5000. Do you need an HTTP proxy? % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": context deadline exceeded (Client.Timeout exceeded while awaiting headers) % docker image push 10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib:latest The push refers to repository \[10.0.50.2:5000/python/ubuntu/hub/python3.10-no-corelib\] Get "https://10.0.50.2:5000/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 1 reply 6 months agoView thread Those are internal IP address. Gotta log into bastion and make a tunnel or something. If you're trying to do it from outside. But why not using Jenkins for this I was in VPN after first attempt Does it work like that? I don't know. I tried to build a Hub image within the repo - cancelled after 20mins building - thought it pointed to need to have it pre-built in container repository Well you can put that in the Jenkins pipeline anyway, it should not change between builds if the dicker file doesbt change Dicker\* Docker\* Autocorrector damnit Lil Lol October 13th, 2022 FYI: the docker registry is available via (wireguard) VPN as well: https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud -> “Private Docker Registry” (edited)  Could I have a login to access both: Nomad: https://nomad-ui-dev.skoobe.online:4443/ui/jobs Consul: https://consul-ui-dev.skoobe.online:4443/ui/dc1/services they are all the same as staging I think omad nomad:sdhK2S\_A.13Du54 for all of them This too? Dashboard (basic auth protected): https://opensearch-ui-dev.skoobe.online:4443/app/home#/ dont know Great thanks October 21st, 2022 FYI There's a range tree instruction that ingests all keys from subdirs you may find useful {{ range tree "skoobe/configuration/queues/sqs" }} {{ .Key | toUpper }}={{ .Value }} {{ end }} oh neat might need a tweak - it's rendering to local/env with the file as a prefix ENVIRONMENT/ENVIRONMENT=production ENVIRONMENT/SERVER\_ENVIRONMENT=staging ENVIRONMENT/SERVICES\_ENV=STAGING Still cool to nknw October 26th, 2022 @Arndt Jenssen I'm trying to use the nomad template {source = construct in place of  template {date <<-EOF   as many of the hub job will share a similar setup template { # disable\_file\_sandbox = true source = "local/hub-nomad-tpl" # \* Must specify a destination for the template destination = "local/env" env = true } however I'm at pains to get the .tpl file to the ephemeral disk I have a preceding stanza with artifact { source = "https://bb\_user:app\_password@bitbucket.org/skoobe/skoobe-hub/src/hcloud-stg/nomad/hcloud/nomad.tpl" destination = "local/hub-nomad-tpl" } https://bb\_user:app\_password@bitbucket.org/skoobe/skoobe-hub/src/hcloud-stg/nomad/hcloud/nomad.tpl is fine from a browser (already logged in), could a solution be for a skoobe bitbucket app password with read rights on skoobe-hub or something else? Source file in Skoobe / skoobe-hub nomad/hcloud/nomad.tpl Lines Size 2.22 kB 1  data = <<-EOF 2   3                {{ range tree "skoobe/configuration/global" }} 4                {{ .Key | toUpper 5  ... skoobe-hub | Added by Bitbucket Cloud hm, no idea why this is not working. bitbucket app passwords should be fine, altough the jenkins instances are using ssh keys for bitbucket access. I added you to the bitbucket admin group for your further investigation pleasure, but I’d suggest not to spend too much time on it and use the things that are already working. I have hub-ui running, but it's unreachable I figure now I needed a 00-traefik-stg.nomad entry # for HUB UI 4556 "--entryPoints.hub-ui.address=:4556", what's the procedure for doing a rolling deployment of Traefik you shouldn’t need that i think, we don’t declare a service for hub i dont think we just need to expose the port, 4556 in the network stanza right>? or maybe the task stanza network { port "http" { static = 4556 host\_network = "private" } } inside the group staza and then host = "nom.cli-stg-1.skoobe.online"? 2 replies Last reply 5 months agoView thread and this for the service stanza : service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", \] check { type = "tcp" interval = "10s" timeout = "2s" } I think that should workl even better would be to use a real hostname, but this needs to be created in DNS via terraform config. then one could do this in the hub-ui service stanza (and can get rid of the static port) service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", "traefik.http.routers.${NOMAD\_JOB\_NAME}.rule=Host(\`hub-ui-${ENVIRONMENT}.skoobe.online\`)" \] have to change traisn now… Will create that this evening, also for account-ui. Just go with the static port option first .. That's what I did It's not hard to change later created hub-ui-\[env\] and account-ui-\[env\] dns hosts.. 3 replies Last reply 5 months agoView thread October 27th, 2022 anyone had this error in nomad before trying to launch a job: Missing: kv.block(skoobe/credentials/aws-access-key), kv.block(skoobe/credentials/aws-region), kv.block(skoobe/credentials/aws-secret-key), and 1 more (pretty sure these keys are in the consul) nevermind. had - instead of \_ (dumb guy) wrt hub-ui I have issues getting my head round the operation of Traefik and the tag interpolation for it The job is successfully deployed - job shows ip4 to be http://195.201.141.25:8060 .nomad file stanza is: resources { cpu = 100 memory = 1024 network { port "http" { static = 8060 host\_network = "private" } } } service { name = "${NOMAD\_JOB\_NAME}" port = "http" tags = \[ "traefik.enable=true", "traefik.http.routers.${NOMAD\_JOB\_NAME}.entrypoints=http-vpn", "traefik.http.routers.${NOMAD\_JOB\_NAME}.rule=Host(\`hub-ui-${ENVIRONMENT}.skoobe.online\`)" \] Traefik shows what appears to be a valid routing entry (img2) I cannot access Ui either at http://hub-ui-stg.skoobe.online:8060/ or http://195.201.141.25:8060 When hub-ui entry was added to common.tfvars   - has a rollout taken place? subdomain\_prefixes = \["nomad-ui", "consul-ui", "traefik-ui", "sqs-ui", "opensearch-ui", "carbon-ui", "mysql", "app-services", "account-api", "www", "account-ui", "hub-ui"\] 3 files  has the hub-ui nomad config still a static port assigned? ah, yes, can see it above change “static = 8060” to “to = 8060" - then you could access it via http://hub-ui-stg.skoobe.online October 28th, 2022 is the whole hcloud stg environment broken for you guys? I ran the traefik nomad job to add: # 4558: App Services for HUB "--entryPoints.hub-app-services.address=:4558", I confirmed that it was added to the dashboard, so I assumed all was well - then went for lunch. I can connect to the DBs seems to be up again @Arndt Jenssen Did you add some external entrypoint for mux? no, because it is TLS terminating itself. it runs against its own Hetzner load balancer which round-robins to all nomad instances (so it should run as a system type job). its descibed on https://jira4skoobe.atlassian.net/wiki/spaces/IT/pages/3222831109/Hetzner+Cloud under “MUX access” so it bypasses Traefik completely.. https://console.hetzner.cloud/projects/1297723/loadbalancers
> 
> 1 / 1

It seems that this Slack channel has been discussing various technical topics related to the company's infrastructure and services. Some of the main subjects discussed include:

1.  Setting up a TCP load balancer for MUX via Terraform on Hetzner Cloud.
2.  Deploying and configuring new production database servers, with details about their hardware specifications and setup.
3.  Working with Nomad templates and addressing issues with pulling artifacts from Bitbucket.
4.  Setting up and configuring Hub-UI with Traefik, including adding DNS hosts and modifying Nomad configurations.
5.  Debugging issues with missing Consul keys and issues accessing the Hub-UI.
6.  Discussion about making changes to the Traefik Nomad job to add an entry point for Hub App Services.

The team members have been collaborating and sharing information to help each other address challenges and improve the infrastructure.

1 / 1